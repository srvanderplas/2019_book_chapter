---
title: "Machine Learning in Firearms Examination"
author: ["Alicia Carriquiry, Heike Hofmann, Xiao Hui Tai, Susan VanderPlas", "This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement #70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia."]
date: "2019-04-30"
output: 
  pdf_document: 
    latex_engine: xelatex
    keep_tex: yes
header-includes:
  - \usepackage{subfig}
  - \usepackage{amsmath,amssymb}
  - \usepackage[utf8]{inputenc}
  - \usepackage{natbib}
bibliography: refs.bib
---

```{r setup, include=FALSE, echo = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.align = "center", dev = "CairoPDF", cache = T)
library(imager)
library(x3ptools)
library(bulletxtrctr)
library(cowplot)
library(gridExtra)
library(tidyverse)
```

## Introduction

When a crime is committed, the evidence collected by crime scene investigators may come in different forms. The criminal may have left a droplet of blood from which a DNA profile can be extracted. Or s/he may have shed textile fibers that can be characterized by their physical and chemical attributes. Or, most commonly, the perpetrator of the crime left a shoe print, or a finger print, or some other form of what is known as pattern evidence. In the forensics community, pattern evidence includes hand-writing, firearms and tool marks, finger prints and shoe prints, tire tread prints, blood spatter, and anything else that comes in the form of an image.

Regardless of the type of evidence available to investigators, a question of interest typically involves the issue of source; could the suspect’s shoe have left the bloody print next to the victim? Did the saliva on the cigarette butt come from the suspect’s mouth? Notice that even if the answers to these questions is yes, this would not imply that the suspect is the perpetrator of the crime, since there may be innocent reasons that explain the suspect’s contact with the crime scene. But determining whether the suspect could have been the source of the evidence is an important first step in most forensic analysis.

If DNA can be extracted from the evidence, or when the evidence can be characterized using quantitative measurements, then we can often resort to standard statistical methods to compare two samples and test the hypothesis of same source. But when the evidence consists of a pattern, then the usual modeling framework is no longer an option. Figure 1 illustrates the problem. The left panel shows a latent print found at a crime scene and the right panel shows a reference print obtained from the suspect’s shoe. We wish to determine whether the shoe could have been the source of the latent print, and if possible, we would like to attach an estimate of uncertainty to the conclusion.

```{r latent-shoe-print, fig.cap = "Latent shoe print and reference outsole image of a suspect's shoe.", out.width = "\\textwidth", echo = F, fig.width = 8, fig.height = 5}
lst <- lapply(c("figure/crime_scene_90.jpg", "figure/database_60.png"), load.image)
lst[[1]] <- lst[[1]] %>% imrotate(-90) %>% pad(. , 321, "y", pos = 1, val = rep(1, spectrum(.))) %>% pad(10, "x", val = rep(1, spectrum(.)))
imappend(lst, "x") %>% imrotate(-90) %>% plot(axes = F)
```

Comparing images such as those shown in Figure 1 is challenging for several reasons. First, we do not have, as in the case of DNA, a generative model that would enable us to reduce the dimensionality of the images and establish a formal testing approach. Second, a typical image has tens of thousands of pixels, and further, a pixel-by-pixel comparison is not robust to changes in scale, rotation, and translation. Finally, it is not obvious whether the information contained in the image can be summarized into a few measurements that could then be used to carry out the comparison.

As a consequence, the state of the art for evaluating and interpreting most types of pattern evidence is a subjective approach, where forensic examiners rely mostly on their training and experience. In a typical evaluation, the examiner will compare the two samples side by side, and will decide whether the number of common features in both images is large enough to conclude that the two samples could have a common source. Depending on the specific area, examiners may rely on instruments such as a comparison microscope, but the final decision is almost entirely subjective.

While some forensic examiners have years and even decades of experience, the fact that the evaluation of pattern evidence continues to be subjective is problematic. For most of the pattern disciplines, there is no universal agreement of what constitutes “similar enough”. All fingerprint examiners make use of a common set of specific minutiae when comparing two prints for example, but there is no rule that says that the number of minutiae in agreement must be at least $x$ before the prints are deemed to be similar. Another challenge that arises from the subjective evaluation of evidence is that it is also difficult to estimate the rate of errors, not only for individual examiners but for the discipline as a whole. Experience is no substitute for experimentation, where ground truth is known, and the fact that an examiner may have never been challenged in court does not mean that he or she has never made a mistake. Finally, when assessments are subjective, it is possible that two examiners looking at the same evidence will reach different conclusions, or even that the same examiner will make a different assessment when examining the same evidence on two different occasions.

Because of these and several other concerns, the National Research Council of the United States assembled a panel of experts who in 2009 published a report that was strongly critical of all forensic disciplines with the exception of DNA analysis of single-donor or simple mixture samples.1 The report singled out pattern evidence as particularly lacking in scientific validity and rife with subjectivity, and called for an immediate and sustained research effort to shore up pattern evidence by – among other recommendations – developing the scientific and statistical framework that underpin any scientific discipline.

In the decade since, machine learning methods have been developed to try to address this issue of subjectivity in the assessment of pattern evidence.

## Pattern Recognition and Machine Learning

Pattern recognition is a scientific discipline ``concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories". In statistics, algorithms for pattern recognition are sometimes referred to as learning algorithms and can be used for inference or for prediction.

Algorithms for pattern recognition can be grouped into two major classes, the class of supervised learning methods and the class of unsupervised learning methods. Supervised learning algorithms assume that we have available a large set of units with known labels, called a training set. In statistical parlance, we use the term response instead of label, and predictors or independent variables instead of features. From this training set, the algorithm “learns” how the features relate to the labels, or in other words, produces an estimate of the function  that describes the association between features  and label . The idea is that once the algorithm has been trained, meaning that an estimate  has been obtained, the algorithm can be used to predict the labels for previously unseen objects. 

A good algorithm strikes a balance between the competing objectives of optimizing its performance on the training data while at the same time, minimizing classification errors when presented with new units. This is known as the variance-bias tradeoff; more flexible models, that fit the training data very well, have high variance, because even a small change in the data might result in a large change in the fitted model. On the other hand, models that fit the training data less closely, may be more robust to changes in the observations, but tend to exhibit a larger bias. An example of a supervised learning method is discriminant analysis, introduced to statisticians by Fisher (1936). Often, when labels are discrete, the learning algorithm is known as a classifier because it is typically used to classify units into different classes. Neural nets (NN) and convolutional neural nets (CNN) are also examples of supervised learning algorithms (Goodfellow et al. 2016), although they are sometimes referred to as unsupervised methods.

Unsupervised learning algorithms, in contrast, do not depend on the availability of training data with known labels. Here, the assumption is that we observe a set of measurements or features  on a large sample of units, but not the corresponding response or label. Instead, the algorithm itself determines the combination of features that best represent a class and that maximize the probability of assigning the correct label to a new unit. Unsupervised learning algorithms find patterns in the training data and in this sense, are similar to data mining or knowledge data discovery (KDD), methods that are used more commonly in business applications. While this type of methods have been used in forensic applications (e.g. Kong et al. 2019), we focus on the use of learning algorithms to address questions of source.

## Firearm Identification

Firearm identification is one area of forensics in which learning algorithms have been used to address questions of source. As currently practiced, firearms examination is a combination of science and art. Current standard practice is for a firearms examiner to compare two bullet samples - one recovered from a crime scene, the other a test shot fired by a suspect’s gun – side by side under a comparison microscope. The examiner then subjectively determines whether the samples are distinguishable or indistinguishable, or if the comparison is inconclusive.

This determination is made based on comparisons of the marks left on a bullet (or its cartridge case) after it has been fired. Marks are made in a number of ways. One cause is the rifling of a gun barrel (the spiral groves that impart rotation to the bullet as it is expelled), which create a pattern of groove engraved areas and land engraved areas (lands being the areas between each groove). In the United States, where firearms are readily accessible, the annual number of gun-related crimes is in the hundreds of thousands, and about two thirds of all murders are committed with a gun (United States Bureau of Federal Investigations 2017). Therefore, most crime labs employ one or more experts in firearms identification, who can compare bullets or cartridge cases found in a crime scene with test shots obtained from the suspect’s gun, or with samples recovered from a different crime scene.

Firearms identification – or ballistics, as it is sometimes incorrectly called – has been practiced for a long time. Wilson and Wilson (2003) tell the story of Confederate General Jackson, who was killed in battle in 1863, during the Civil War. The slug recovered from his body was examined and found to be a 0.67-caliber ball, the type of ammunition used by Confederate, not by Union soldiers. This first application of forensic ballistics investigation led to the surprising conclusion that General Jackson had been killed by one of his own men. Today, forensic examiners will seek to ascertain the type of ammunition used in a crime, but they will also try to establish whether a bullet was fired by a specific gun. Figure ... shows the engraved areas left behind by the lands and grooves of the barrel on the bullet. Small imperfections in the surface of the barrel create striations on the bullet as it leaves the gun. Marks are also imprinted to the base of the cartridge case by the breech face of the gun at the time it is fired. The right panel in Figure \ref{fig:bullet-cartridge-images} shows a cartridge case with breech face marks. While the class or rifling characteristics of a gun can be readily determined from the pattern of land engraved areas and grooves on a bullet, or from the shape of the firing pin impression on the base of a cartridge case, it is the microscopic sub-class and individual striations that are used in firearms identification.

```{r bullet-cartridge-images, fig.cap = "Striations on fired bullets (left) and marks imprinted by the breech face and firing pin on the primer of a spent cartridge case (right)", out.width = "\\textwidth"}
lst <- c("figure/Bullet.png", "figure/Cartridge.png") %>%
  lapply(., load.image)

lst[[2]] <- imresize(lst[[2]], scale = pmin(height(lst[[1]])/height(lst[[2]]), width(lst[[1]])/width(lst[[2]]))) %>% 
  pad(10, axes = "x", pos = 1, val = rep(1, spectrum(.)))
imappend(lst, "x") %>% plot(axes = F)
```

As currently practiced, firearms examination is a combination of science and art. But in the past 20 years or so there has been a push to use imaging technology to obtain actual measurements of the surface topology of land engraved areas on bullets and of cartridge case bases that might enable more objective analysis of the similarity between two samples. One of the first proponents of measuring features of the surface topology of bullets were De Kinder and Bonfanti.3 The authors used a laser profilometer to scan bullets and obtain measurements of the distance and depth of striations. They then computed a correlation between two aligned sets of features to quantify the similarity between two bullets. Since then, several new methods that rely on 2D and 3D imaging of bullets and cartridge cases have been proposed to quantify the similarity between two items. In 2017 Hare et al. proposed using supervised learning algorithms to construct a similarity score that can be used to compare bullets.4

For most rifling types, fired bullets exhibit a sequence of land engraved areas separated by grooves. The striations that are used to compare bullets appear on the land engraved areas, so the measurements are obtained by scanning each area individually. Measurements on a land engraved area consist of heights on an $x-y$ grid in micron-level increments. The exact resolution at which images are taken depends on the microscope. In Hare et al., scans made available through the NIST Ballistics Research Database were used. These scans are taken at a resolution of $1.5625 \mu m \times 1.5625\mu m$. The total area that is captured from each land engraved area is approximately $2.2 mm \times 0.6 mm$, and the data are the $x-y-z$  coordinates of each point on the grid. Figure 3 shows the 3D scan of a land engraved area of a bullet fired from a Smith & Wesson firearm. The most informative striations are located close to the bottom of the bullet, in the area shaded in red in the figure.

```{r generate-bullet-renders}
x3pfile <- read_x3p(file = "data/HS36-B1-B1-L1.x3p")
x3pfileb2 <- read_x3p(file = "data/HS36-B1-B2-L4.x3p")

# Interpolate a bit using imager - fill in holes in places where they're small
x3pfile2 <- x3pfile
x3pfile2$surface.matrix %<>% as.cimg() %>% inpaint(5) %>% as.matrix()

na_mask <- x3pfile$surface.matrix %>% is.na() %>% as.cimg() %>% clean(20)
na_mask2 <- na_mask %>% grow(30) %>% shrink(27)

x3pfile2$surface.matrix[na_mask2] <- NA

crosscut <- x3p_crosscut(x3pfile2)
nomask <- x3p_add_mask(x3pfile2, matrix("#cd7f32", ncol = 3577, nrow = 1023))
hline <- x3p_add_hline(nomask, yintercept = unique(crosscut$y - 100), color = "darkred", size = 40)
# image_x3p(hline)
# x3p_snapshot(file = "figure/HS36-Bullet-With-Crosscut.png")

x3pdf <- x3p_to_df(x3pfile)
x3pdf_filter <- filter(x3pdf, y > 150)
x3pfilter <- df_to_x3p(x3pdf_filter)

grooves <- list(groove = c(110, 2120))
signature <- cc_get_signature(crosscut, grooves)

# image_x3p(x3pfilter, multiply = 3)
# 
# pmat <- structure(c(0.9976, -0.00916, 0.06750, 
# 0, 0.0681, 0.1071, -0.9919, 
# 0, 0.00185, 0.99420, 0.1075, 0, 
# 0, 0, 0, 1), .Dim = c(4L, 4L))
# rgl::par3d(userMatrix = pmat)
# x3p_snapshot(file = "figure/HS36-Bullet-With-Crosscut_rotate90.png")
```

```{r bullet-scan-crosscut-image, fig.cap = "Scanned surface of a bullet land engraved area. Striations in the area shaded in red are most informative.", out.width = ".8\\textwidth"}
knitr::include_graphics("figure/HS36-Bullet-With-Crosscut.png")
```
The algorithm proposed in Hare et al. focuses on the average height of striations observed on a set of consecutive cross-sections of the land engraved area at a value of $y$ where striations appear to be stable. Figure 4 shows the same scanned area as in Figure 3, but viewed as a cross-section where $y$ is fixed. The bottom panel is a single-pixel representation of the cross-section shown in the upper panel. In both panels, it is evident where the land engraved area begins and ends, but in some cases, finding the grooves in an automated way is challenging.

One other attribute that is immediately noticeable from the figure is that the dominant structure in the cross-sectional representation of the land engraved area is the curvature of the surface of the bullet. Therefore, the first step toward extracting a signature from a bullet is to subtract the curvature from the image via a Loess fit. (Loess stands for “locally estimated scatterplot smoothing”, and is an approach to fit a smooth curve to a set of points by fitting a set of simple models to local subsets of the data.) The residuals from the Loess function represent the actual striations and are used to construct a signature. Figure 5 shows the fitted curve in blue in the top panel, and the Loess residuals in the bottom panel. Note that the range of the residuals is approximately 6 $\mu$m. For some combinations of ammunition and gun, the striations are less pronounced.

```{r bullet-scan-crosscut-side-view, fig.cap =  "Sideways view of the scan from Figure 3 (top), with height measurements across the red shaded area (bottom).", out.width = ".8\\textwidth"}
plot_grid(
  ggdraw() + draw_image("figure/HS36-Bullet-With-Crosscut_rotate90.png"), 
  ggplot(aes(x = x, y = value), data = filter(x3pdf, abs(y - 329.595) < 10)) +
    geom_point(size = 0.5) +
    theme_bw() +
    coord_fixed(ratio = 3) +
    xlab("x") +
    ylab("z"),
  nrow = 2
)
```

```{r bullet-remove-curvature, fig.cap = "Loess fit to remove curvature shown in blue (top). Residuals from the Loess fit (bottom) show the relative heights of the engraved striae.", out.width = ".8\\textwidth", fig.width = 8, fig.height = 4}
p1 <- ggplot(aes(x = x, y = value), data = filter(x3pdf, abs(y - 329.595) < 2)) +
  geom_point(size = 0.5) +
  geom_smooth(aes(x = x, y = value), data = filter(x3pdf, abs(y - 329.595) < 10, x > 110, x < 2120)) +
  theme_bw() +
  xlim(c(0, 2300)) +
  xlab("Relative Location (in \u03BCm)") +
  ylab("Scan Height (in \u03BCm)") +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), 
        axis.text.y = element_text(angle = 90), 
        plot.margin = margin(b = 0, unit = "pt"))

p2 <- ggplot(aes(x = x, y = sig), data = signature) +
  geom_line(size = 0.5) +
  theme_bw() +
  xlim(c(0, 2300)) +
  xlab("Relative Location (in \u03BCm)") +
  ylab("Signature (in \u03BCm)") +
  theme(plot.margin = margin(b = 0, unit = "pt"), 
        axis.text.y = element_text(angle = 90))

grid.arrange(p1, p2, nrow = 2, heights = c(1, 1.2))
```

Once the signatures from two bullets have been extracted, it is possible to compare the overlapped signatures using quantitative methods. For example, it is possible to compute a cross-correlation between two signatures, or to measure differences in the height of peaks or depth of valleys. Indeed, there are several different features that can be quantified and used to decide whether the signatures are similar enough to suggest that the bullets may have been fired from the same gun. Figure 6 shows the overlain signatures for two bullets that were fired from the same gun. Even though the signatures are not exactly the same, the similarities are enough to suggest that they may have a common source.

```{r overlaid-signatures, fig.cap = "Overlaid signatures of two bullets fired from the same gun.", out.width = ".8\\textwidth"}
crosscut2 <- x3p_crosscut(x3pfileb2)
grooves2 <- list(groove = c(175, 2100))
signature2 <- cc_get_signature(crosscut2, grooves2)


align <- sig_align(signature$sig, signature2$sig)
aligndf <- align$lands %>%
  gather(key = type, value = sig, -x)

ggplot(aes(x = x, y = sig, linetype = type), data = aligndf) +
  geom_line(size = 0.5) +
  theme_bw() +
  xlim(c(0, 2300)) +
  xlab("Relative Location (in μm)") +
  ylab("Signatures (in μm)") +
  theme(plot.margin = margin(b = 0, unit = "pt")) +
  scale_linetype_discrete(guide = F)
```

\citet{hare2017algorithmic} considered various features; to decide whether any of them would serve to accurately determine whether two bullets could have been fired from the same gun, they carried out the following experiment. First, they sourced 3D images of bullets that were fired by Hamby et al. using 10 consecutively rifled 9mm Ruger barrels.  Next they constructed all possible pairs of images of land engraved areas; some pairs corresponded to the same land on bullets fired from the same barrel, others corresponded to either different pairs of land engraved areas on bullets fired from the same barrels or to pairs of land engraved areas on bullets fired from different barrels. Then, for each pair of images Hare et al. computed the value of each of seven features; the distribution of values of each of the features among pairs of known matching land engraved areas and among pairs of known non-matching land engraved areas are shown in \autoref{fig:empirical-feature-dist}.

```{r features-setup}

features <- read.csv("data/H44-old-features.csv")

# f2 <- features %>%
#   tidyr::separate(land1, c("barrel1", "bullet1", "land1"), "-") %>%
#   tidyr::separate(land2, c("barrel2", "bullet2", "land2"), "-")

f2nest <- features %>% filter(!(barrel1==barrel2 & bullet1 == bullet2)) %>%
  group_by( bullet2, barrel1, barrel2, bullet1) %>%  nest()

rotate <- function(x, lag) {
  n <- length(x)
  rep(x, 2)[lag+1:n]
}

f3nest <- f2nest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f=function(d) {
    dt = xtabs(ccf ~land1+land2, data=d)
    drow <- dim(dt)[2]
    1:drow %>% purrr::map_dbl(.f=function(i) {
      mean(diag(dt[,rotate(1:drow,i)]))
    }) %>% max()
  }),
  sam_rf = data %>% purrr::map_dbl(.f=function(d) {
    dt = xtabs(rfscore ~land1+land2, data=d)
    drow <- dim(dt)[2]
    1:drow %>% purrr::map_dbl(.f=function(i) {
      mean(diag(dt[,rotate(1:drow,i)]))
    }) %>% max()
  }),
  sam_rf_phase = data %>% purrr::map_dbl(.f=function(d) {
    dt = xtabs(ccf ~land1+land2, data=d)
    drow <- dim(dt)[2]
    1:drow %>% purrr::map_dbl(.f=function(i) {
      mean(diag(dt[,rotate(1:drow,i)]))
    }) %>% which.max()
  }),
  KM_sam = sam_rf > 0.5
)

inphase <- function(land1, land2, sam_rf_phase, ...) {
  as.numeric(land2) %% 6 == (as.numeric(land1) + sam_rf_phase) %% 6
}

tmp <- unnest(f3nest) %>%
  mutate(inphase = purrr::pmap_lgl(., inphase),
         KM = inphase & KM_sam)
```

```{r empirical-feature-dist, fig.cap = "Empirical feature distributions", fig.width = 8, fig.height = 4, out.width = ".8\\textwidth"}
tmp %>% 
  select(KM, cms, non_cms, matches, mismatches, D, ccf, H.H) %>%
  set_names(c("Matches", "CMS", "CNMS", "# Matches", "# Non-Matches", "D", "CCF", "H.H")) %>%
  mutate(Matches = ifelse(Matches, "Match (KM)", "Non-Match (KNM)")) %>%
  gather(key = "key", value = "value", -Matches) %>%
  ggplot(aes(x = value, fill = Matches)) + 
  theme_bw() + 
  geom_density(color = "black", alpha = .75, adjust = 2) + 
  scale_fill_manual(values = c("Non-Match (KNM)" = "#73abd0", "Match (KM)" = "#bde2eb")) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  ylab("Density") + 
  theme(legend.position = c(1, 0), legend.justification = c(1, 0), axis.title.x = element_blank())
```
Ideally, we would like to find one or more feature that can be used to discriminate between known matching and known non-matching pairs, but it appears from  \autoref{fig:empirical-feature-dist} that none of the seven features meets the criterion. This is because in every case there is significant overlap between the distributions of values among the two sets of images. \citet{hare2017algorithmic} proposed instead that features be combined into a single score using a random forest – an ensemble learning method for classification or regression based on the idea of decision trees \citep{breimanRandomForests2001}. For classification, the random forest outputs the most likely class for each unit, and each unit is scored between 0 and 1, where larger scores denote a higher degree of similarity. In fact, random forest scores can be thought of as the empirical probability of same class computed for each pair of images.

For the case of the Hamby bullets, the random forest produced a perfect classification of all pairs into the two classes. The separation between the values of the scores among known matching and known non-matching pairs was complete, as is shown in \autoref{fig:score-dist}.
```{r score-dist, fig.cap = "Random forest scores for the Hamby et al. (2009) known matching pairs of land engraved areas (dark blue) and known non-matching pairs (light blue).", fig.width = 8, fig.height = 4, out.width = ".8\\textwidth"}
f3nest %>% select(KM = KM_sam, sam_rf) %>%
  mutate(method = "Forest") %>%
  mutate(KM2 = c("KNM", "KM")[KM + 1] %>% as.factor()) %>%
  ggplot(aes(x = sam_rf, y = KM2, color = KM2, shape = KM2)) +
  facet_grid(method~.) +
  geom_jitter() +
  scale_color_manual(guide = F, values = c("KNM" = "#73abd0", "KM" = "#bde2eb")) +
  scale_shape_discrete(guide = F) +
  theme_bw() +
  theme(axis.title.y = element_blank()) +
  scale_x_continuous("Scores", limits = c(0, 1))
```

One limitation of learning algorithms is that they tend to over-fit the training data; as a consequence, an algorithm’s performance when classifying a new set of units can be dramatically worse and result in large mis-classification errors. To guard against over-fitting, it is possible to set aside a portion of the training data that can then be used as a test dataset, but even then, the mis-classification error tends to be under-estimated.

To explore whether the random forest fitted to the Hamby et al. bullets has good classification performance when used to classify pairs of land engraved areas from bullets fired by guns of different make and model, and when the ammunition is also made by a different manufacturer, we applied the model to thousands of pairs of known matching and known non-matching land engraved areas obtained from crime laboratories across the United States. Results have been promising; the random forest correctly determined every pair of bullets in every study when the gun barrel had conventional (rather than polygonal) rifling and striations were reasonably well marked, and when bullets were not coated with a polymer that flakes on contact with the barrel. 

We have found that some guns including Sig Sauer and Berettas impart shallow engravings, and we are currently revising the algorithm so that it will adapt to differences in depth of striations. Figure 9 shows the results observed in one such test set. The set consisted of three reference shots from each of eight barrels, and 10 fired rounds of unknown provenance. The test set was open which means that it was possible that some of the test shots were not fired by any of the eight study barrels and that one or more of the barrels may have fired none of the 10 test shots. In the figure, we show a composite random forest score computed as the average of the scores obtained for each land to land comparison.

We have found that some guns including Sig Sauer and Berettas impart shallow engravings, and we are currently revising the algorithm so that it will adapt to differences in depth of striations. Figure 9 shows the results observed in one such test set. The set consisted of three reference shots from each of eight barrels, and 10 fired rounds of unknown provenance. The test set was open which means that it was possible that some of the test shots were not fired by any of the eight study barrels and that one or more of the barrels may have fired none of the 10 test shots. In the figure, we show a composite random forest score computed as the average of the scores obtained for each land to land comparison.
