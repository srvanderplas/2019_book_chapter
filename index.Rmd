---
title: "Machine Learning in Firearms Examination"
author: ["Alicia Carriquiry, James Hamby, Heike Hofmann, Xiao Hui Tai, Susan VanderPlas", "This work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement #70NANB15H176 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, University of California Irvine, and University of Virginia."]
date: "2019-04-30"
output: 
  pdf_document: 
    citation_package: natbib
    keep_tex: yes
header-includes:
  - \usepackage{subfig}
  - \usepackage{amsmath,amssymb}
  - \usepackage[utf8]{inputenc}
bibliography: refs.bib
---

```{r setup, include=FALSE, echo = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.align = "center", 
                      dev = "CairoPDF", cache = T)
library(imager)
library(x3ptools)
library(bulletxtrctr)
library(cowplot)
library(gridExtra)
library(tidyverse)
```

# Introduction

<!-- Probably need to replace at least some of this with a history/lit review... -->

When a crime is committed, the evidence collected by crime scene investigators may come in different forms. The criminal may have left a droplet of blood from which a DNA profile can be extracted. Or s/he may have shed textile fibers that can be characterized by their physical and chemical attributes. Or, most commonly, the perpetrator of the crime left a shoe print, or a finger print, or some other form of what is known as pattern evidence. In the forensics community, pattern evidence includes hand-writing, firearms and tool marks, finger prints and shoe prints, tire tread prints, blood spatter, and anything else that comes in the form of an image.

Regardless of the type of evidence available to investigators, a question of interest typically involves the issue of source; could the suspect’s shoe have left the bloody print next to the victim? Did the saliva on the cigarette butt come from the suspect’s mouth? Notice that even if the answers to these questions is yes, this would not imply that the suspect is the perpetrator of the crime, since there may be innocent reasons that explain the suspect’s contact with the crime scene. But determining whether the suspect could have been the source of the evidence is an important first step in most forensic analysis.

If DNA can be extracted from the evidence, or when the evidence can be characterized using quantitative measurements, then we can often resort to standard statistical methods to compare two samples and test the hypothesis of same source. But when the evidence consists of a pattern, then the usual modeling framework is no longer an option. \autoref{fig:latent-shoe-print} illustrates the problem. The left panel shows a latent print found at a crime scene and the right panel shows a reference print obtained from the suspect’s shoe. We wish to determine whether the shoe could have been the source of the latent print, and if possible, we would like to attach an estimate of uncertainty to the conclusion.

```{r latent-shoe-print, fig.cap = "Latent shoe print and reference outsole image of a suspect's shoe.", out.width = "\\textwidth", echo = F, fig.width = 8, fig.height = 5}
lst <- lapply(c("figure/crime_scene_90.jpg", "figure/database_60.png"), load.image)
lst[[1]] <- lst[[1]] %>% imrotate(-90) %>% pad(. , 321, "y", pos = 1, val = rep(1, spectrum(.))) %>% pad(10, "x", val = rep(1, spectrum(.)))
imappend(lst, "x") %>% imrotate(-90) %>% plot(axes = F)
```

Comparing images such as those shown in \autoref{fig:latent-shoe-print} is challenging for several reasons. First, we do not have, as in the case of DNA, a generative model that would enable us to reduce the dimensionality of the images and establish a formal testing approach. Second, a typical image has tens of thousands of pixels, and further, a pixel-by-pixel comparison is not robust to changes in scale, rotation, and translation. Finally, it is not obvious whether the information contained in the image can be summarized into a few measurements that could then be used to carry out the comparison.

As a consequence, the state of the art for evaluating and interpreting most types of pattern evidence is a subjective approach, where forensic examiners rely mostly on their training and experience. In a typical evaluation, the examiner will compare the two samples side by side, and will decide whether the number of common features in both images is large enough to conclude that the two samples could have a common source. Depending on the specific area, examiners may rely on instruments such as a comparison microscope, but the final decision is almost entirely subjective.

While some forensic examiners have years and even decades of experience, the fact that the evaluation of pattern evidence continues to be subjective is problematic. For most of the pattern disciplines, there is no universal agreement of what constitutes “similar enough”. All fingerprint examiners make use of a common set of specific minutiae when comparing two prints for example, but there is no rule that says that the number of minutiae in agreement must be at least $x$ before the prints are deemed to be similar. Another challenge that arises from the subjective evaluation of evidence is that it is also difficult to estimate the rate of errors, not only for individual examiners but for the discipline as a whole. Experience is no substitute for experimentation, where ground truth is known, and the fact that an examiner may have never been challenged in court does not mean that he or she has never made a mistake. Finally, when assessments are subjective, it is possible that two examiners looking at the same evidence will reach different conclusions, or even that the same examiner will make a different assessment when examining the same evidence on two different occasions.

Because of these and several other concerns, the \citeauthor{StrengtheningForensicScience2009} assembled a panel of experts who in 2009 published a report that was strongly critical of all forensic disciplines with the exception of DNA analysis of single-donor or simple mixture samples. The report singled out pattern evidence as particularly lacking in scientific validity and rife with subjectivity, and called for an immediate and sustained research effort to shore up pattern evidence by – among other recommendations – developing the scientific and statistical framework that underpin any scientific discipline.

In the decade since, machine learning methods have been developed to try to address this issue of subjectivity in the assessment of pattern evidence.

## Pattern Recognition and Machine Learning

Pattern recognition is a scientific discipline ``concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories". In statistics, algorithms for pattern recognition are sometimes referred to as learning algorithms and can be used for inference or for prediction.

Algorithms for pattern recognition can be grouped into two major classes, the class of supervised learning methods and the class of unsupervised learning methods. Supervised learning algorithms assume that we have available a large set of units with known labels, called a training set. In statistical parlance, we use the term response instead of label, and predictors or independent variables instead of features. From this training set, the algorithm “learns” how the features relate to the labels, or in other words, produces an estimate of the function  that describes the association between features  and label . The idea is that once the algorithm has been trained, meaning that an estimate  has been obtained, the algorithm can be used to predict the labels for previously unseen objects. 

A good algorithm strikes a balance between the competing objectives of optimizing its performance on the training data while at the same time, minimizing classification errors when presented with new units. This is known as the variance-bias tradeoff; more flexible models, that fit the training data very well, have high variance, because even a small change in the data might result in a large change in the fitted model. On the other hand, models that fit the training data less closely, may be more robust to changes in the observations, but tend to exhibit a larger bias. An example of a supervised learning method is discriminant analysis, introduced to statisticians by \citet{fisherUSEMULTIPLEMEASUREMENTS1936}. Often, when labels are discrete, the learning algorithm is known as a classifier because it is typically used to classify units into different classes. Neural nets (NN) and convolutional neural nets (CNN) are also examples of supervised learning algorithms \citep{Goodfellow2016}, although they are sometimes referred to as unsupervised methods.

Unsupervised learning algorithms, in contrast, do not depend on the availability of training data with known labels. Here, the assumption is that we observe a set of measurements or features  on a large sample of units, but not the corresponding response or label. Instead, the algorithm itself determines the combination of features that best represent a class and that maximize the probability of assigning the correct label to a new unit. Unsupervised learning algorithms find patterns in the training data and in this sense, are similar to data mining or knowledge data discovery (KDD), methods that are used more commonly in business applications. While this type of methods have been used in forensic applications \citep{kongCrossDomainImageMatching2019a}, we focus on the use of learning algorithms to address questions of source.

# Firearm Identification

Firearm identification is one area of forensics in which learning algorithms have been used to address questions of source. As currently practiced, firearms examination is a combination of science and art. Current standard practice is for a firearms examiner to compare two bullet samples - one recovered from a crime scene, the other a test shot fired by a suspect’s gun – side by side under a comparison microscope. The examiner then subjectively determines whether the samples are distinguishable or indistinguishable, or if the comparison is inconclusive.

This determination is made based on comparisons of the marks left on a bullet (or its cartridge case) after it has been fired. Marks are made in a number of ways. One cause is the rifling of a gun barrel (the spiral groves that impart rotation to the bullet as it is expelled), which create a pattern of groove engraved areas and land engraved areas (lands being the areas between each groove). In the United States, where firearms are readily accessible, the annual number of gun-related crimes is in the hundreds of thousands, and about two thirds of all murders are committed with a gun \citep{fbi2017}. Therefore, most crime labs employ one or more experts in firearms identification, who can compare bullets or cartridge cases found in a crime scene with test shots obtained from the suspect’s gun, or with samples recovered from a different crime scene.

## A Brief History of Firearms Identification

Firearms identification – or ballistics, as it is sometimes incorrectly called – has been practiced for a long time.  \citet{wilson2003written} tell the story of Confederate General Jackson, who was killed in battle in 1863, during the Civil War. The slug recovered from his body was examined and found to be a 0.67-caliber ball, the type of ammunition used by Confederate, not by Union soldiers. This first application of forensic firearms investigation led to the surprising conclusion that General Jackson had been killed by one of his own men. 
<!-- Not the first - 1835 Bow Street Runners case in London (homemade ammunition matched to a mold) -->

In \citeyear{hallMissileWeapon1900}, a paper by \citeauthor{hallMissileWeapon1900} published in the Buffalo Medical Journal detailed methods for test firing weapons (into a bag of meal) for the purposes of comparing the engravings resulting from the rifling. While it is not clear from the paper whether the comparisons made are class characteristics (e.g. the spacing of land and groove engraved areas) or individual characteristics, this represents the first known foray into what might be termed ``modern" examinations of fired bullets. In 1902, testimony concerning the markings on bullets was admitted into evidence \citep{commonwealthVbest} in a criminal case for the first time, and in 1907, fired cartridge cases were also used as evidence \citep{brownsvilleRiot}. In 1912, photographic comparisons were used to examine individualized markings of land and groove engraved areas; enlarged photographs were also used to compare cartridge case marks. In the 1920s, the idea that individualizing characteristics are present on fired bullets and cartridges became more widely accepted across the country, accompanied by an increase in court cases involving evidence from bullets and cartridge cases. 

The process of comparing the striations of bullets recovered from a crime scene to test-fired bullets from recovered weapons became much easier in 1925 with the invention of the comparison microscope, which allowed two bullets to be examined simultaneously and manually aligned\citep{afte-examiner-training}. The same year, a series of two articles in the Saturday Evening Post described the potential uses of firearms identification in popular press \citep{BulletsExpertWitness,BulletsSilentWitness}, increasing the public's awareness of the potential of firearm forensics. Five years later, Goddard would testify about the firearms used in the Valentine's Day Massacre \citep{goddardValentineDayMassacre1930}; this testimony included analysis of the cartridge cases recovered from the scene as well as the markings left on the bullets. 

Firearms investigation became more formalized in the 1930s, with textbooks published on the subject in both the UK and the United States \citep{burrard1934identification,hatcherTextbookFirearmsInvestigation1935,gunther1935identification}. The 1930s also saw the foundation of a number of laboratories focused on the scientific examination of forensic evidence, a trend that continued into the 1940s. In the 1950s and 1960s, the field began to move toward greater quantification of firearms evidence, with the introduction of the striagraph \citep{davis1968introduction}, an early forerunner of laser and digital scanning of bullets, and with the landmark paper by \citet{biasottiStatisticalStudyIndividual1959} which used visual features of pairwise bullet comparisons to produce a quantitative description of the strength of the match between the two bullets. 

Today, forensic examiners will begin by identifying the type of ammunition used in a crime, and then establish whether a bullet was fired by a specific gun. The left panel of \autoref{fig:bullet-cartridge-images} shows the engraved areas left behind by the lands and grooves of the barrel on the bullet. Small imperfections in the surface of the barrel create striations on the bullet as it leaves the gun. Marks are also imprinted to the base of the cartridge case by the breech face of the gun at the time it is fired. The right panel in Figure \ref{fig:bullet-cartridge-images} shows a cartridge case with breech face marks. While the class or rifling characteristics of a gun can be readily determined from the pattern of land engraved areas and grooves on a bullet, or from the shape of the firing pin impression on the base of a cartridge case, it is the microscopic sub-class and individual striations that are used in firearms identification.

```{r bullet-cartridge-images, fig.cap = "Striations on fired bullets (left) and marks imprinted by the breech face and firing pin on the primer of a spent cartridge case (right)", out.width = "\\textwidth"}
lst <- c("figure/Bullet.png", "figure/Cartridge.png") %>%
  lapply(., load.image)

lst[[2]] <- imresize(lst[[2]], scale = pmin(height(lst[[1]])/height(lst[[2]]), width(lst[[1]])/width(lst[[2]]))) %>% 
  pad(10, axes = "x", pos = 1, val = rep(1, spectrum(.)))
imappend(lst, "x") %>% plot(axes = F)
```


As currently practiced, firearms examination is a combination of science and art. Some methods have been developed to automatically match images of bullet land engraved areas or cartridge cases \citep{gardnerComputerIdentificationBullets1978,geradtsImageMatchingAlgorithms2001}, but in the past 20 years or so there has been a push to use imaging technology to obtain actual measurements of the surface topology of land engraved areas on bullets and of cartridge case bases that might enable more objective analysis of the similarity between two samples. One of the first proponents of measuring features of the surface topology of bullets were \citet{dekinderAutomatedComparisonsBullet1999}. The authors used a laser profilometer to scan bullets and obtain measurements of the distance and depth of striations. They then computed a correlation between two aligned sets of features to quantify the similarity between two bullets. Since then, several new methods that rely on 2D and 3D imaging of bullets and cartridge cases have been proposed to quantify the similarity between two items\citep{tongFiredCartridgeCase2014,fischerDigitalCrimeScene2014,chumbleyValidationToolMark2010}. In 2017, \citeauthor{hare2017algorithmic} proposed using supervised learning algorithms to construct a similarity score that can be used to compare bullets, and \citet{taiFullyAutomaticMethod2018} constructed a similar scoring algorithm for cartridge case comparison.

For most rifling types, fired bullets exhibit a sequence of land engraved areas separated by grooves. The striations that are used to compare bullets appear on the land engraved areas, so the measurements are obtained by scanning each area individually. Measurements on a land engraved area consist of heights on an $x-y$ grid in micron-level increments. The exact resolution at which images are taken depends on the microscope. In \citeauthor{hare2017algorithmic}, scans made available through the NIST Ballistics Research Database were used. These scans are taken at a resolution of $1.5625 \mu m \times 1.5625\mu m$. The total area that is captured from each land engraved area is approximately $2.2 mm \times 0.6 mm$, and the data are the $x-y-z$  coordinates of each point on the grid. Figure 3 shows the 3D scan of a land engraved area of a bullet fired from a Smith & Wesson firearm. The most informative striations are located close to the bottom of the bullet, in the area shaded in red in the figure.

```{r generate-bullet-renders}
x3pfile <- read_x3p(file = "data/HS36-B1-B1-L1.x3p")
x3pfileb2 <- read_x3p(file = "data/HS36-B1-B2-L4.x3p")

# Interpolate a bit using imager - fill in holes in places where they're small
x3pfile2 <- x3pfile
x3pfile2$surface.matrix %<>% as.cimg() %>% inpaint(5) %>% as.matrix()

na_mask <- x3pfile$surface.matrix %>% is.na() %>% as.cimg() %>% clean(20)
na_mask2 <- na_mask %>% grow(30) %>% shrink(27)

x3pfile2$surface.matrix[na_mask2] <- NA

crosscut <- x3p_crosscut(x3pfile2)
nomask <- x3p_add_mask(x3pfile2, matrix("#cd7f32", ncol = 3577, nrow = 1023))
hline <- x3p_add_hline(nomask, yintercept = unique(crosscut$y - 100), color = "darkred", size = 40)
# image_x3p(hline)
# x3p_snapshot(file = "figure/HS36-Bullet-With-Crosscut.png")

x3pdf <- x3p_to_df(x3pfile)
x3pdf_filter <- filter(x3pdf, y > 150)
x3pfilter <- df_to_x3p(x3pdf_filter)

grooves <- list(groove = c(110, 2120))
signature <- cc_get_signature(crosscut, grooves)

# image_x3p(x3pfilter, multiply = 3)
# 
# pmat <- structure(c(0.9976, -0.00916, 0.06750, 
# 0, 0.0681, 0.1071, -0.9919, 
# 0, 0.00185, 0.99420, 0.1075, 0, 
# 0, 0, 0, 1), .Dim = c(4L, 4L))
# rgl::par3d(userMatrix = pmat)
# x3p_snapshot(file = "figure/HS36-Bullet-With-Crosscut_rotate90.png")
```

```{r bullet-scan-crosscut-image, fig.cap = "Scanned surface of a bullet land engraved area. Striations in the area shaded in red are most informative.", out.width = ".8\\textwidth"}
knitr::include_graphics("figure/HS36-Bullet-With-Crosscut.png")
```

The algorithm proposed in \citeauthor{hare2017algorithmic} focuses on the average height of striations observed on a set of consecutive cross-sections of the land engraved area at a value of $y$ where striations appear to be stable; \autoref{fig:bullet-scan-crosscut-image} shows the selected value of $y$ for a land engraved area scan. \autoref{fig:bullet-scan-crosscut-side-view} shows the same scanned area as in \autoref{fig:bullet-scan-crosscut-image}, but viewed as a cross-section where $y$ is fixed. The bottom panel is a single-pixel representation of the cross-section shown in the upper panel. In both panels, it is evident where the land engraved area begins and ends, but in some cases, finding the grooves in an automated way is challenging.

One other attribute that is immediately noticeable from the figure is that the dominant structure in the cross-sectional representation of the land engraved area is the curvature of the surface of the bullet. Therefore, the first step toward extracting a signature from a bullet is to subtract the curvature from the image via a Loess fit. (Loess stands for “locally estimated scatterplot smoothing”, and is an approach to fit a smooth curve to a set of points by fitting a set of simple models to local subsets of the data.) The residuals from the Loess function represent the actual striations and are used to construct a signature. \autoref{fig:bullet-remove-curvature} shows the fitted curve in blue in the top panel, and the Loess residuals in the bottom panel. Note that the range of the residuals is approximately 6 $\mu$m. For some combinations of ammunition and gun, the striations are less pronounced.

```{r bullet-scan-crosscut-side-view, fig.cap =  "Sideways view of the scan from Figure 3 (top), with height measurements across the red shaded area (bottom).", out.width = ".8\\textwidth"}
plot_grid(
  ggdraw() + draw_image("figure/HS36-Bullet-With-Crosscut_rotate90.png"), 
  ggplot(aes(x = x, y = value), data = filter(x3pdf, abs(y - 329.595) < 10)) +
    geom_point(size = 0.5) +
    theme_bw() +
    coord_fixed(ratio = 3) +
    xlab("x") +
    ylab("z"),
  nrow = 2
)
```

```{r bullet-remove-curvature, fig.cap = "Loess fit to remove curvature shown in blue (top). Residuals from the Loess fit (bottom) show the relative heights of the engraved striae.", out.width = ".8\\textwidth", fig.width = 8, fig.height = 4}
p1 <- ggplot(aes(x = x, y = value), data = filter(x3pdf, abs(y - 329.595) < 2)) +
  geom_point(size = 0.5) +
  geom_smooth(aes(x = x, y = value), data = filter(x3pdf, abs(y - 329.595) < 10, x > 110, x < 2120)) +
  theme_bw() +
  xlim(c(0, 2300)) +
  xlab("Relative Location (in \u03BCm)") +
  ylab("Scan Height (in \u03BCm)") +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), 
        axis.text.y = element_text(angle = 90), 
        plot.margin = margin(b = 0, unit = "pt"))

p2 <- ggplot(aes(x = x, y = sig), data = signature) +
  geom_line(size = 0.5) +
  theme_bw() +
  xlim(c(0, 2300)) +
  xlab("Relative Location (in \u03BCm)") +
  ylab("Signature (in \u03BCm)") +
  theme(plot.margin = margin(b = 0, unit = "pt"), 
        axis.text.y = element_text(angle = 90))

grid.arrange(p1, p2, nrow = 2, heights = c(1, 1.2))
```

Once the signatures from two bullets have been extracted, it is possible to compare the overlapped signatures using quantitative methods. For example, it is possible to compute a cross-correlation between two signatures, or to measure differences in the height of peaks or depth of valleys. Indeed, there are several different features that can be quantified and used to decide whether the signatures are similar enough to suggest that the bullets may have been fired from the same gun. \autoref{fig:overlaid-signatures} shows the overlain signatures for two bullets that were fired from the same gun. Even though the signatures are not exactly the same, the similarities are enough to suggest that they may have a common source.

```{r overlaid-signatures, fig.cap = "Overlaid signatures of two bullets fired from the same gun.", out.width = ".8\\textwidth"}
crosscut2 <- x3p_crosscut(x3pfileb2)
grooves2 <- list(groove = c(175, 2100))
signature2 <- cc_get_signature(crosscut2, grooves2)


align <- sig_align(signature$sig, signature2$sig)
aligndf <- align$lands %>%
  gather(key = type, value = sig, -x)

ggplot(aes(x = x, y = sig, linetype = type), data = aligndf) +
  geom_line(size = 0.5) +
  theme_bw() +
  xlim(c(0, 2300)) +
  xlab("Relative Location (in μm)") +
  ylab("Signatures (in μm)") +
  theme(plot.margin = margin(b = 0, unit = "pt")) +
  scale_linetype_discrete(guide = F)
```

\citet{hare2017algorithmic} considered various features; to decide whether any of them would serve to accurately determine whether two bullets could have been fired from the same gun, they carried out the following experiment. First, they sourced 3D images of bullets that were fired by Hamby et al. using 10 consecutively rifled 9mm Ruger barrels.  Next they constructed all possible pairs of images of land engraved areas; some pairs corresponded to the same land on bullets fired from the same barrel, others corresponded to either different pairs of land engraved areas on bullets fired from the same barrels or to pairs of land engraved areas on bullets fired from different barrels. Then, for each pair of images \citeauthor{hare2017algorithmic} computed the value of each of seven features; the distribution of values of each of the features among pairs of known matching land engraved areas and among pairs of known non-matching land engraved areas are shown in \autoref{fig:empirical-feature-dist}.

```{r features-setup}

features <- read.csv("data/H44-old-features.csv")

# f2 <- features %>%
#   tidyr::separate(land1, c("barrel1", "bullet1", "land1"), "-") %>%
#   tidyr::separate(land2, c("barrel2", "bullet2", "land2"), "-")

f2nest <- features %>% filter(!(barrel1==barrel2 & bullet1 == bullet2)) %>%
  group_by( bullet2, barrel1, barrel2, bullet1) %>%  nest()

rotate <- function(x, lag) {
  n <- length(x)
  rep(x, 2)[lag+1:n]
}

f3nest <- f2nest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f=function(d) {
    dt = xtabs(ccf ~land1+land2, data=d)
    drow <- dim(dt)[2]
    1:drow %>% purrr::map_dbl(.f=function(i) {
      mean(diag(dt[,rotate(1:drow,i)]))
    }) %>% max()
  }),
  sam_rf = data %>% purrr::map_dbl(.f=function(d) {
    dt = xtabs(rfscore ~land1+land2, data=d)
    drow <- dim(dt)[2]
    1:drow %>% purrr::map_dbl(.f=function(i) {
      mean(diag(dt[,rotate(1:drow,i)]))
    }) %>% max()
  }),
  sam_rf_phase = data %>% purrr::map_dbl(.f=function(d) {
    dt = xtabs(ccf ~land1+land2, data=d)
    drow <- dim(dt)[2]
    1:drow %>% purrr::map_dbl(.f=function(i) {
      mean(diag(dt[,rotate(1:drow,i)]))
    }) %>% which.max()
  }),
  KM_sam = sam_rf > 0.5
)

inphase <- function(land1, land2, sam_rf_phase, ...) {
  as.numeric(land2) %% 6 == (as.numeric(land1) + sam_rf_phase) %% 6
}

tmp <- unnest(f3nest) %>%
  mutate(inphase = purrr::pmap_lgl(., inphase),
         KM = inphase & KM_sam)
```

```{r empirical-feature-dist, fig.cap = "Empirical feature distributions", fig.width = 8, fig.height = 4, out.width = ".8\\textwidth"}
tmp %>% 
  select(KM, cms, non_cms, matches, mismatches, D, ccf, H.H) %>%
  set_names(c("Matches", "CMS", "CNMS", "# Matches", "# Non-Matches", "D", "CCF", "H.H")) %>%
  mutate(Matches = ifelse(Matches, "Match (KM)", "Non-Match (KNM)")) %>%
  gather(key = "key", value = "value", -Matches) %>%
  ggplot(aes(x = value, fill = Matches)) + 
  theme_bw() + 
  geom_density(color = "black", alpha = .75, adjust = 2) + 
  scale_fill_manual(values = c("Non-Match (KNM)" = "#73abd0", "Match (KM)" = "#bde2eb")) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  ylab("Density") + 
  theme(legend.position = c(1, 0), legend.justification = c(1, 0), axis.title.x = element_blank())
```
Ideally, we would like to find one or more feature that can be used to discriminate between known matching and known non-matching pairs, but it appears from  \autoref{fig:empirical-feature-dist} that none of the seven features meets the criterion. This is because in every case there is significant overlap between the distributions of values among the two sets of images. \citet{hare2017algorithmic} proposed instead that features be combined into a single score using a random forest – an ensemble learning method for classification or regression based on the idea of decision trees \citep{breimanRandomForests2001}. For classification, the random forest outputs the most likely class for each unit, and each unit is scored between 0 and 1, where larger scores denote a higher degree of similarity. In fact, random forest scores can be thought of as the empirical probability of same class computed for each pair of images.

For the case of the Hamby bullets, the random forest produced a perfect classification of all pairs into the two classes. The separation between the values of the scores among known matching and known non-matching pairs was complete, as is shown in \autoref{fig:score-dist}.
```{r score-dist, fig.cap = "Random forest scores for the Hamby et al. (2009) known matching pairs of land engraved areas (dark blue) and known non-matching pairs (light blue).", fig.width = 8, fig.height = 4, out.width = ".8\\textwidth"}
f3nest %>% select(KM = KM_sam, sam_rf) %>%
  mutate(method = "Forest") %>%
  mutate(KM2 = c("KNM", "KM")[KM + 1] %>% as.factor()) %>%
  ggplot(aes(x = sam_rf, y = KM2, color = KM2, shape = KM2)) +
  facet_grid(method~.) +
  geom_jitter() +
  scale_color_manual(guide = F, values = c("KNM" = "#73abd0", "KM" = "#bde2eb")) +
  scale_shape_discrete(guide = F) +
  theme_bw() +
  theme(axis.title.y = element_blank()) +
  scale_x_continuous("Scores", limits = c(0, 1))
```

One limitation of learning algorithms is that they tend to over-fit the training data; as a consequence, an algorithm’s performance when classifying a new set of units can be dramatically worse and result in large mis-classification errors. To guard against over-fitting, it is possible to set aside a portion of the training data that can then be used as a test dataset, but even then, the mis-classification error tends to be under-estimated.

To explore whether the random forest fitted to the Hamby et al. bullets has good classification performance when used to classify pairs of land engraved areas from bullets fired by guns of different make and model, and when the ammunition is also made by a different manufacturer, we applied the model to thousands of pairs of known matching and known non-matching land engraved areas obtained from crime laboratories across the United States. Results have been promising; the random forest correctly determined every pair of bullets in every study when the gun barrel had conventional (rather than polygonal) rifling and striations were reasonably well marked, and when bullets were not coated with a polymer that flakes on contact with the barrel. 

We have found that some guns including Sig Sauer and Berettas impart shallow engravings, and we are currently revising the algorithm so that it will adapt to differences in depth of striations.  \autoref{fig:score-match-plot} shows the results observed in one such test set. Each pairwise comparison is represented as a square, with the center of the square shaded to represent the composite match score for the six lands on each bullet; the border of the square is colored orange if the bullets are from the same source. In the test set shown in \autoref{fig:score-match-plot}, there are three reference shots from each of eight barrels, and 10 fired rounds of unknown provenance. The test set was open, which means that it was possible that some of the test shots were not fired by any of the eight study barrels and that one or more of the barrels may have fired none of the 10 test shots; in this case, questioned bullets Q, Y, and Z did not originate from the 8 barrels in the study.

```{r score-match-plot, fig.cap = "Random forest scores of the similarity between 10 test shots and 24 reference shots from eight 9mm barrels.", out.width = ".8\\textwidth"}
features <- read.csv("data/pd-features.csv.gz")
features <- features %>% mutate(
  lys = overlap*signature_length*1000/.645,
  cms = cms2 *lys*.645/1000
)
# change labels for land 3 and land 4 of bullet B3 in P7
features <- features %>% mutate(
  first = as.character(first),
  first = replace(first, first=="Gun 1-P7/B3/L4.dat", "XXX"),
  first = replace(first, first=="Gun 1-P7/B3/L3.dat", "Gun 1-P7/B3/L4.dat"),
  first = replace(first, first=="XXX", "Gun 1-P7/B3/L3.dat")
)

features <- features %>% mutate(
  KM = replace(KM, first %in% c("Gun 1-P7/B3/L3.dat", "Gun 1-P7/B3/L4.dat"), FALSE)
)
features <- features %>% mutate(
  KM = ifelse(first == "Gun 1-P7/B3/L3.dat", (second == "Unknown 1-J/L3.dat"), KM),
  KM = ifelse(first == "Gun 1-P7/B3/L4.dat", (second == "Unknown 1-J/L4.dat"), KM)
)



f2 <- features %>%
  separate(first, into = c("foo1", "foo2", "barrel1", "bullet1", "land1", "foo3"), remove = FALSE) %>%
  separate(second, into = c("foo4", "foo5", "bullet2", "land2", "foo6"), remove = FALSE) %>%
  select(-foo1, -foo2, -foo3, -foo4, -foo5, -foo6)


unknowns <- read.csv("data/pd-features-unknown.csv")
unknowns <- unknowns %>% filter(bullet1 != bullet2)
unknowns <- unknowns %>% mutate(
  bullet2 = gsub("Unknown 1-", "", bullet2),
  bullet1 = gsub("Unknown 1-", "", bullet1)
)
f3 <- rbind(
  f2 %>%
    select(
      b1, b2, barrel1, bullet1,
      land1, bullet2, land2, ccf, rfscore, cms, KM
    ) %>%
    mutate(barrel2 = "Unknown"),
  unknowns %>%
    select(
      b1, b2, bullet1,
      land1, bullet2, land2, ccf, rfscore, cms, 
    ) %>%
    mutate(barrel1 = "Unknown", barrel2 = "Unknown", KM = FALSE)
)

pdnest <- f3 %>% group_by(bullet2, barrel1, bullet1) %>% nest()

pdnest <- pdnest %>% mutate(
  sam_ccf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$ccf)
    max(scores)
  }),
  sam_rf = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$rfscore)
    max(scores)
  }),
  KM = data %>% purrr::map_lgl(.f = function(d) {
    any(d$KM)
  }),
  sam_cms = data %>% purrr::map_dbl(.f = function(d) {
    scores <- bulletxtrctr::compute_average_scores(d$land1, d$land2, d$cms)
    max(scores)
  })
)

pdnest <- pdnest %>%
  mutate(
    bullet2 = factor(bullet2, levels = rev(c("N", "B", "E", "T", "H", "J", "K", "Q", "Y", "Z"))),
    bullet1 = factor(bullet1, levels = c("B1", "B2", "B3", rev(levels(bullet2))))
  )
p3 <- pdnest %>%
  ggplot(aes(y = bullet2, x = bullet1, fill = sam_rf)) +
  geom_tile(size = 1) +
  facet_grid(. ~ barrel1, scales = "free", space = "free") +
  ylab("Questioned bullets") +
  xlab("Known bullets and Questioned bullets") +
  scale_fill_gradient2("RF score ",
    low = "darkgrey",
    high = "darkorange", midpoint = 0.45, limits = c(0, 1)
  ) +
  scale_colour_manual("Same source", values = "darkorange") +
  scale_y_discrete() +
  theme_bw() +
  theme(legend.position = "bottom") +
  #  coord_equal() +
  geom_tile(aes(colour = TRUE),
    size = .5,
    data = filter(pdnest, KM)
  ) +
  guides(colour = guide_legend(override.aes = list(fill = NA)))


pdnest$samesource <- pdnest$KM

p3
```

We used a threshold of 0.5 for the random forest score to categorize pairs of bullets into "same" and "different" gun. A row of three orange squares indicates that the corresponding questioned bullet matched all three of the reference shots fired by the barrel indicated in the column. In all cases, the algorithm classified the questioned bullets correctly. It correctly concluded, for example, that the last three questioned shots labeled $Q, Y, Z$ were not similar enough to any of the reference shots to declare a match, and that the barrel labeled $U10$ did not fired any of the questions rounds.

However, this type of methodology must undergo extensive additional testing and validation before it can be implemented in practice as there are several issues that still need addressing. 
First, from working with different sets of bullets for which we know ground truth, we have found that there is variation in the optimal score threshold for categorizing pairs of bullets as “same” or “different” . This may be a consequence of the fact that the random forest we have been applying to those sets of images was trained only once and only on a small training set. As more 3D images become available, it will be important to train the forest on a more diverse set that perhaps includes combinations of bullets and guns of the same caliber but manufactured by different companies. 
Second, fully automating the process is difficult, because the detection of the grooves is challenging. At this moment, it is still necessary to carefully check the automatically identified groove locations, which not only slows down the process but also increases the chances for errors. 
Finally, adoption of this new technology in working crime labs will require significant changes in protocols and in the way in which firearm examiners interpret and present their evidence. Thus, we are not likely to see these methodologies used in practice for several more years.


# Back to the source

The machine learning methods we have discussed to quantify the similarity between two items do not, by themselves, suffice to address the question of source. If a questioned bullet and a test shot from the suspect’s gun are similar, all we can say is that the suspect’s gun cannot be excluded as the source of the questioned bullet. To be able to conclude that the suspect’s gun and no other fired the questioned round, we would need to also show that the degree of similarity we observed is probative: we observed the same degree of similarity when two bullets were fired from the same gun. The same type of reasoning applies to all other forms of evidence.

In the case of DNA profiles, we know that barring laboratory error, a match between two samples is probative, because no two individuals (except identical twins) have the same set of alleles at every marker. For fingerprints, it is also assumed that each person is born with an individual pattern, so that perfect prints from two different individuals should be distinguishable. For all other types of evidence, we have no means to compute a probative value, and therefore, we do not know whether a high degree of similarity between two items is indication or not of a common source. Until recently, forensic examiners in reports or testimony were able to focus on the similarity between two objects and conclude that high degree of similarity implied common source. But the blunders that led to questioning the validity of most forensic disciplines also led the public and the scientific community to revisit assumptions such as the uniqueness and repeatability of striations of rounds fired by a gun. As a result, juries and law professionals today are more likely to expect some discussion about the probative value of a ``match" from forensic experts.
In the absence of a statistical model that may permit estimating the likelihood of a pattern under different assumptions about provenance, it may be possible to compute an empirical estimate of the frequency with which a given degree of similarity between two items can be expected when the items have a common source and when they do not. \autoref{fig:l2l-scores} illustrates this idea using firearms as an example.

```{r l2l-scores, fig.width = 8, fig.height = 4, out.width = ".8\\textwidth", fig.cap = "Empirical distributions of random forest scores for pairs of land engraved area scans known to have been produced from the same land (orange) and known to have originated from different lands (gray). The vertical lines in the top and bottom panels represent the score that was obtained when comparing a questioned with a reference scan of a land engraved area."}
# scores_all <- bind_rows(
#   data_frame(score=rbeta(10000, 2, 10), class = "Non-mates"),
#   data_frame(score=rbeta(10000, 4, 2), class = "Mates")
# )

sample_scores <- tibble(panel = c("Low score", "High score"), score = c(0.3, 0.8))

f3_plot <- f3 %>%
  mutate(KM = c("Non-mates", "Mates")[KM + 1])

# This uses land-to-land data because it's at least slightly more variable - bullet-to-bullet comparisons are so separated this calculation is not really feasible. 
ggplot() +
  geom_density(aes(rfscore, color = KM, fill=KM), data = f3_plot, alpha=0.5) +
  scale_fill_manual("", values = c("orange", "grey50")) + 
  scale_color_manual("", values = c("orange", "grey50")) + 
  geom_vline(aes(xintercept = score), data = sample_scores) + 
  facet_grid(panel~.) + 
  scale_x_continuous("Score") + 
  theme_bw() + 
  theme(axis.title.y = element_blank(), legend.position = c(1, 1), legend.justification = c(1, 1), legend.title = element_blank(), legend.background  = element_rect(fill = "transparent"))

```

The empirical distributions of similarity scores shown in orange and in gray in \autoref{fig:l2l-scores} represent the values of the score that could have been obtained when comparing bullets fired by the same or by different guns, respectively. Suppose that these distributions of scores were obtained from a very large number of pairs of bullets representing the population of guns and ammunition of a certain caliber. A crime is committed and a bullet is recovered from the scene. The suspect’s gun is test fired and the bullet in evidence is compared to the test shot using the algorithm that produced the background scores. Suppose that the resulting score is 0.8, as in the top panel in \autoref{fig:l2l-scores}. Visual inspection suggests that a score of 0.8 appears to be likely if the two bullets were fired from the same gun and unlikely otherwise. We could also calculate the probability of observing a score of 0.8 under each of the two empirical distributions. Regardless of the method, we would conclude that the two bullets were likely fired by the same gun. If instead the similarity score was 0.3 as in the bottom panel, it would not be possible to reach a conclusion regarding source with any degree of confidence. The ratio of the heights of both distributions for a score of 0.3 indicate that it is about twice as likely to observe that score if bullets were fired by different guns, which is only weak evidence in favor of the hypothesis of different source. 

The conclusion is that in order to assess the probative value of a match between two pieces of evidence, we need to have extensive background information about the population from which those two pieces of evidence might have originated. Thus, the construction of those reference databases must become a priority for the scientific community in general and for the forensics community in particular.

## Some final thoughts

This year we celebrate a decade since the publication of the 2009 NRC report on the state of the forensic sciences in the United States. While progress has been slow, there is no question that the broader scientific community is now more interested and more involved in the development of new technologies to improve the analysis and interpretation of forensic evidence. At the same time, there has been an awakening to the fact that in the past, some of the conclusions offered by forensic experts have exceeded the limits of the science and the data. In parallel with the ongoing research in forensic science and statistics, the Department of Justice of the United States has begun an effort to curb the language that can be used by federal forensic experts and to develop a common language for reporting the results of forensic analysis that is consistent across disciplines.

<!-- Need to change this to be more firearms specific? -->
<!-- Machine learning has the potential to greatly improve the way in which forensic scientists evaluate pattern and other types of evidence, and researchers in CSAFE are pioneering the use of learning methods in forensic applications with impressive results. As well as the bullet matching work discussed here, Kong et al. use a convolutional neural net to automatically determine the mark and model of the shoe that left a print at a crime scene and multi-channel deep features for matching a questioned image to reference images from a database.7 Park and Carriquiry8 show that at least for the data that are available, a random forest exhibits significantly fewer mis-classification errors for glass fragments than the methods currently proposed as the standard by the American Society for Testing and Materials.9, 10 -->

No algorithm will completely replace humans for the analysis and interpretation of evidence, but with the appropriate background databases, and after extensive testing and validation, algorithms can help eliminate some of the subjectivity that permeates most forensic disciplines and can serve to at least approximate the degree of uncertainty in forensic conclusions.

# References
